version: 2.0
jobs:
  build:
    working_directory: ~/hw-kafka-client
    docker:
      - image: fpco/stack-build:latest

      - image: confluentinc/cp-zookeeper
        environment:
          ZOOKEEPER_CLIENT_PORT: 2181

      - image: confluentinc/cp-kafka
        environment:
          KAFKA_ZOOKEEPER_CONNECT: "localhost:2181"
          KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://localhost:9092"
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

    environment:
      LD_LIBRARY_PATH: /usr/local/lib

    steps:
      - checkout
      - restore_cache:
          keys:
            - dot-stack-{{ checksum "stack.yaml" }}-{{ checksum "hw-kafka-client.cabal" }}
            - dot-stack
      - restore_cache:
          key: stack-work-{{ checksum "stack.yaml" }}
      - restore_cache:
          key: librdkafka-{{ checksum "scripts/build-librdkafka.sh" }}

      - run:
          name: Build librdkafka
          command: ./scripts/build-librdkafka.sh

      - run:
          name: Build librdkafka
          command: ./scripts/build-librdkafka.sh

      - run: stack setup
      - run: stack build --test --no-run-tests --haddock --no-haddock-deps

      - run: ./scripts/copy-docs.sh

      - run:
          name: Running unit tests
          command: stack test

      - save_cache:
          key: dot-stack-base
          paths:
            - ~/.stack

      - save_cache:
          key: dot-stack-{{ checksum "stack.yaml" }}-{{ checksum "hw-kafka-client.cabal" }}
          paths:
            - ~/.stack

      - save_cache:
          key: stack-work-{{ checksum "stack.yaml" }}
          paths: ~/hw-kafka-client/.stack-work

      - save_cache:
          key: librdkafka-{{ checksum "scripts/build-librdkafka.sh" }}
          paths: ~/hw-kafka-client/.librdkafka

      - store_artifacts:
          path: /tmp/doc
